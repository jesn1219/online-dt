{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import transformers\n",
    "\n",
    "from decision_transformer.models.model import TrajectoryModel\n",
    "from decision_transformer.models.trajectory_gpt2 import GPT2Model\n",
    "import math\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch import distributions as pyd\n",
    "\n",
    "\n",
    "class TanhTransform(pyd.transforms.Transform):\n",
    "    domain = pyd.constraints.real\n",
    "    codomain = pyd.constraints.interval(-1.0, 1.0)\n",
    "    bijective = True\n",
    "    sign = +1\n",
    "\n",
    "    def __init__(self, cache_size=1):\n",
    "        super().__init__(cache_size=cache_size)\n",
    "\n",
    "    @staticmethod\n",
    "    def atanh(x):\n",
    "        return 0.5 * (x.log1p() - (-x).log1p())\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return isinstance(other, TanhTransform)\n",
    "\n",
    "    def _call(self, x):\n",
    "        return x.tanh()\n",
    "\n",
    "    def _inverse(self, y):\n",
    "        # We do not clamp to the boundary here as it may degrade the performance of certain algorithms.\n",
    "        # one should use `cache_size=1` instead\n",
    "        return self.atanh(y)\n",
    "\n",
    "    def log_abs_det_jacobian(self, x, y):\n",
    "        # We use a formula that is more numerically stable, see details in the following link\n",
    "        # https://github.com/tensorflow/probability/commit/ef6bb176e0ebd1cf6e25c6b5cecdd2428c22963f#diff-e120f70e92e6741bca649f04fcd907b7\n",
    "        return 2.0 * (math.log(2.0) - x - F.softplus(-2.0 * x))\n",
    "\n",
    "\n",
    "\n",
    "class SquashedNormal(pyd.transformed_distribution.TransformedDistribution):\n",
    "    \"\"\"\n",
    "    Squashed Normal Distribution(s)\n",
    "\n",
    "    If loc/std is of size (batch_size, sequence length, d),\n",
    "    this returns batch_size * sequence length * d\n",
    "    independent squashed univariate normal distributions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, loc, std):\n",
    "        self.loc = loc\n",
    "        self.std = std\n",
    "        self.base_dist = pyd.Normal(loc, std)\n",
    "\n",
    " \n",
    "        transforms = [TanhTransform()]\n",
    "\n",
    "        super().__init__(self.base_dist, transforms)\n",
    "\n",
    "    @property\n",
    "    def mean(self):\n",
    "        mu = self.loc\n",
    "        for tr in self.transforms:\n",
    "            mu = tr(mu)\n",
    "        return mu\n",
    "\n",
    "    def entropy(self, N=1):\n",
    "        # sample from the distribution and then compute\n",
    "        # the empirical entropy:\n",
    "        x = self.rsample((N,))\n",
    "        log_p = self.log_prob(x)\n",
    "\n",
    "        # log_p: (batch_size, context_len, action_dim),\n",
    "        return -log_p.mean(axis=0).sum(axis=2)\n",
    "\n",
    "    def log_likelihood(self, x):\n",
    "        # log_prob(x): (batch_size, context_len, action_dim)\n",
    "        # sum up along the action dimensions\n",
    "        # Return tensor shape: (batch_size, context_len)\n",
    "\n",
    "        x = self.transforms[0](x)        \n",
    "        return self.log_prob(x).sum(axis=2)\n",
    "\n",
    "        # jesnk : this is the log likelihood of the state # jesnk: mark1\n",
    "        for tr in reversed(self.transforms):\n",
    "            x = tr.inv(x)\n",
    "        return self.base_dist.log_prob(x).sum(axis=2)\n",
    "\n",
    "\n",
    "\n",
    "class DiagGaussianActor(nn.Module):\n",
    "    \"\"\"torch.distributions implementation of an diagonal Gaussian policy.\"\"\"\n",
    "\n",
    "    def __init__(self, hidden_dim, act_dim, log_std_bounds=[-5.0, 2.0]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.mu = torch.nn.Linear(hidden_dim, act_dim)\n",
    "        self.log_std = torch.nn.Linear(hidden_dim, act_dim)\n",
    "        self.log_std_bounds = log_std_bounds\n",
    "        def weight_init(m):\n",
    "            \"\"\"Custom weight init for Conv2D and Linear layers.\"\"\"\n",
    "            if isinstance(m, torch.nn.Linear):\n",
    "                nn.init.orthogonal_(m.weight.data)\n",
    "                if hasattr(m.bias, \"data\"):\n",
    "                    m.bias.data.fill_(0.0)\n",
    "\n",
    "        self.apply(weight_init)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        mu, log_std = self.mu(obs), self.log_std(obs)\n",
    "        print(mu.shape, log_std.shape)\n",
    "        log_std = torch.tanh(log_std)\n",
    "        # log_std is the output of tanh so it will be between [-1, 1]\n",
    "        # map it to be between [log_std_min, log_std_max]\n",
    "        log_std_min, log_std_max = self.log_std_bounds\n",
    "        log_std = log_std_min + 0.5 * (log_std_max - log_std_min) * (log_std + 1.0)\n",
    "        std = log_std.exp()\n",
    "        return SquashedNormal(mu, std)\n",
    "\n",
    "\n",
    "from decision_transformer.models.model import TrajectoryModel01\n",
    "class DecisionTransformer02(TrajectoryModel01):\n",
    "\n",
    "    \"\"\"\n",
    "    This model uses GPT to model (Return_1, state_1, action_1, Return_2, state_2, ...)\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    jesnk: this model uses gpt to model (state_1, state_2, ..., state_n)\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_dim,\n",
    "        hidden_size,\n",
    "        state_range,\n",
    "        ordering=0,\n",
    "        act_dim = None,\n",
    "        max_length=None,\n",
    "        eval_context_length=None,\n",
    "        max_ep_len=4096,\n",
    "        state_tanh=True,\n",
    "        stochastic_policy=False,\n",
    "        init_temperature=0.1,\n",
    "        target_entropy=None,\n",
    "        state_mean=None, #jesnk\n",
    "        state_std=None, #jesnk\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(state_dim, max_length=max_length)\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        config = transformers.GPT2Config(\n",
    "            vocab_size=1,  # doesn't matter -- we don't use the vocab\n",
    "            n_embd=hidden_size,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        # note: the only difference between this GPT2Model and the default Huggingface version\n",
    "        # is that the positional embeddings are removed (since we'll add those ourselves)\n",
    "        self.transformer = GPT2Model(config)\n",
    "\n",
    "        self.embed_timestep = nn.Embedding(max_ep_len, hidden_size)\n",
    "        if ordering:\n",
    "            self.embed_ordering = nn.Embedding(max_ep_len, hidden_size)\n",
    "        self.embed_state = torch.nn.Linear(self.state_dim, hidden_size)\n",
    "        \n",
    "        self.embed_ln = nn.LayerNorm(hidden_size)\n",
    "\n",
    "        if stochastic_policy:\n",
    "            self.predict_state = DiagGaussianActor(hidden_size, self.state_dim)\n",
    "        else:\n",
    "            self.predict_state = nn.Sequential(\n",
    "                *(\n",
    "                    [nn.Linear(hidden_size, self.state_dim)]\n",
    "                    + ([nn.Tanh()] if state_tanh else [])\n",
    "                )\n",
    "            )\n",
    "        self.stochastic_policy = stochastic_policy\n",
    "        self.eval_context_length = eval_context_length\n",
    "        self.ordering = ordering\n",
    "        self.state_range = state_range\n",
    "        #self.state_mean = state_mean # jesnk\n",
    "        #self.state_std = state_std # jesnk\n",
    "        #print(f'jesnk: debug: DT01: state_mean:{self.state_mean}, state_std:{self.state_std}')\n",
    "\n",
    "\n",
    "        if stochastic_policy:\n",
    "            self.log_temperature = torch.tensor(np.log(init_temperature))\n",
    "            self.log_temperature.requires_grad = True\n",
    "            self.target_entropy = target_entropy\n",
    "\n",
    "    def temperature(self):\n",
    "        if self.stochastic_policy:\n",
    "            return self.log_temperature.exp()\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        states,\n",
    "        timesteps=None,\n",
    "        ordering=None,\n",
    "        padding_mask=None,\n",
    "    ):\n",
    "\n",
    "        batch_size, seq_length = states.shape[0], states.shape[1] # 512, seq_legnth, \n",
    "\n",
    "        if padding_mask is None:\n",
    "            # attention mask for GPT: 1 if can be attended to, 0 if not\n",
    "            padding_mask = torch.ones((batch_size, seq_length), dtype=torch.long)\n",
    "        if timesteps is None :\n",
    "            timesteps = torch.arange(seq_length, device=states.device).repeat(batch_size, 1)\n",
    "\n",
    "        # embed each modality with a different head\n",
    "        state_embeddings = self.embed_state(states)\n",
    "\n",
    "        if self.ordering:\n",
    "            order_embeddings = self.embed_ordering(timesteps)\n",
    "        else:\n",
    "            order_embeddings = 0.0\n",
    "\n",
    "        state_embeddings = state_embeddings + order_embeddings\n",
    "\n",
    "        # this makes the sequence look like (R_1, s_1, a_1, R_2, s_2, a_2, ...)\n",
    "        # which works nice in an autoregressive sense since states predict actions\n",
    "        # state_embeddings.shape: batch, seq_length, hidden_size\n",
    "        stacked_inputs = (\n",
    "            #torch.stack((state_embeddings), dim=1)\n",
    "            state_embeddings # batch, seq_length, hidden_size\n",
    "            #.permute(0, 2, 1, 3) # batch, seq_length, 1, hidden_size\n",
    "            .reshape(batch_size, 1 * seq_length, self.hidden_size)\n",
    "        )\n",
    "        # stacked_inputs.shape: batch, 1*seq_length, hidden_size\n",
    "        stacked_inputs = self.embed_ln(stacked_inputs)\n",
    "\n",
    "        # to make the attention mask fit the stacked inputs, have to stack it as well\n",
    "        stacked_padding_mask = (\n",
    "            #torch.stack((padding_mask, padding_mask, padding_mask), dim=1)\n",
    "            padding_mask\n",
    "            #.permute(0, 2, 1)\n",
    "            .reshape(batch_size, 1 * seq_length)\n",
    "        )\n",
    "\n",
    "        # we feed in the input embeddings (not word indices as in NLP) to the model\n",
    "        transformer_outputs = self.transformer(\n",
    "            inputs_embeds=stacked_inputs,\n",
    "            attention_mask=stacked_padding_mask,\n",
    "        )\n",
    "        x = transformer_outputs[\"last_hidden_state\"]\n",
    "\n",
    "        # reshape x so that the second dimension corresponds to the original\n",
    "        # returns (0), states (1), or actions (2); i.e. x[:,1,t] is the token for s_t\n",
    "        x = x.reshape(batch_size, seq_length, 1, self.hidden_size).permute(0, 2, 1, 3)\n",
    "        # after reshape : batch, 1, seq_length, hidden_size\n",
    "        # get predictions\n",
    "        # predict next state given state and action\n",
    "\n",
    "        #state_preds = self.predict_state(x[:, 0]) # jesnk: DT1 setting\n",
    "        states_pred = self.predict_state(x[:,0]) # jesnk: must check the index of [:, 0] is correct\n",
    "\n",
    "        # state_preds.shape: batch, seq_length, state_dim\n",
    "        return states_pred\n",
    "\n",
    "    def get_predictions(\n",
    "        self, states, timesteps, num_envs=1, **kwargs\n",
    "    ):\n",
    "        # we don't care about the past rewards in this model\n",
    "        # tensor shape: batch_size, seq_length, variable_dim\n",
    "        states = states.reshape(num_envs, -1, self.state_dim)\n",
    "\n",
    "        # tensor shape: batch_size, seq_length\n",
    "        timesteps = timesteps.reshape(num_envs, -1)\n",
    "\n",
    "        # max_length is the DT context length (should be input length of the subsequence)\n",
    "        # eval_context_length is the how long you want to use the history for your prediction\n",
    "        if self.max_length is not None:\n",
    "            states = states[:, -self.eval_context_length :]\n",
    "            timesteps = timesteps[:, -self.eval_context_length :]\n",
    "\n",
    "            ordering = torch.tile(\n",
    "                torch.arange(timesteps.shape[1], device=states.device),\n",
    "                (num_envs, 1),\n",
    "            )\n",
    "            # pad all tokens to sequence length\n",
    "            padding_mask = torch.cat(\n",
    "                [\n",
    "                    torch.zeros(self.max_length - states.shape[1]),\n",
    "                    torch.ones(states.shape[1]),\n",
    "                ]\n",
    "            )\n",
    "            padding_mask = padding_mask.to(\n",
    "                dtype=torch.long, device=states.device\n",
    "            ).reshape(1, -1)\n",
    "            padding_mask = padding_mask.repeat((num_envs, 1))\n",
    "\n",
    "            states = torch.cat(\n",
    "                [\n",
    "                    torch.zeros(\n",
    "                        (\n",
    "                            states.shape[0],\n",
    "                            self.max_length - states.shape[1],\n",
    "                            self.state_dim,\n",
    "                        ),\n",
    "                        device=states.device,\n",
    "                    ),\n",
    "                    states,\n",
    "                ],\n",
    "                dim=1,\n",
    "            ).to(dtype=torch.float32)\n",
    "            \n",
    "            timesteps = torch.cat(\n",
    "                [\n",
    "                    torch.zeros(\n",
    "                        (timesteps.shape[0], self.max_length - timesteps.shape[1]),\n",
    "                        device=timesteps.device,\n",
    "                    ),\n",
    "                    timesteps,\n",
    "                ],\n",
    "                dim=1,\n",
    "            ).to(dtype=torch.long)\n",
    "\n",
    "            ordering = torch.cat(\n",
    "                [\n",
    "                    torch.zeros(\n",
    "                        (ordering.shape[0], self.max_length - ordering.shape[1]),\n",
    "                        device=ordering.device,\n",
    "                    ),\n",
    "                    ordering,\n",
    "                ],\n",
    "                dim=1,\n",
    "            ).to(dtype=torch.long)\n",
    "        else:\n",
    "            padding_mask = None\n",
    "\n",
    "        state_preds = self.forward(\n",
    "            states,\n",
    "            timesteps,\n",
    "            ordering,\n",
    "            padding_mask=padding_mask,\n",
    "            **kwargs\n",
    "        )\n",
    "        if self.stochastic_policy:\n",
    "            return state_preds\n",
    "        else:\n",
    "            return (\n",
    "                self.clamp_state(state_preds[:, -1])\n",
    "            )\n",
    "\n",
    "    def clamp_state(self, state):\n",
    "        return state.clamp(*self.state_range)\n",
    "    \n",
    "    def save(self, path):\n",
    "        torch.save(self.state_dict(), path)\n",
    "        \n",
    "    def load(self, path):\n",
    "        self.load_state_dict(torch.load(path))\n",
    "        self.eval()\n",
    "        return self\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시퀀스 데이터 예시: [[ 0.49126608  0.50126608  0.51126608  0.52126608  0.53126608]\n",
      " [-0.86012482 -0.85012482 -0.84012482 -0.83012482 -0.82012482]\n",
      " [ 0.94919596  0.95919596  0.96919596  0.97919596  0.98919596]\n",
      " [-0.72323585 -0.71323585 -0.70323585 -0.69323585 -0.68323585]\n",
      " [ 0.33350774  0.34350774  0.35350774  0.36350774  0.37350774]]\n",
      "데이터셋 크기: 10000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# 시퀀스 데이터 생성 함수\n",
    "def create_sequences(seq_length=5, num_sequences=1000, start=-1.0, end=1.0, step=0.01):\n",
    "    sequences = []\n",
    "    while len(sequences) < num_sequences:\n",
    "        start_val = random.uniform(start, end - seq_length * step)\n",
    "        sequence = [start_val + i * step for i in range(seq_length)]\n",
    "        if all(-1 <= x <= 1 for x in sequence):  # 시퀀스 내 모든 값이 -1과 1 사이인지 확인\n",
    "            sequences.append(sequence)\n",
    "    return np.array(sequences)\n",
    "\n",
    "# 데이터셋 클래스\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        #self.sequences = sequences\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.sequences = torch.tensor(sequences, dtype=torch.float).to(device)\n",
    "        \n",
    "        # \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        input_seq = sequence[:-1]  # 입력 시퀀스 (마지막 토큰 제외)\n",
    "        target_seq = sequence[1:]  # 타겟 시퀀스 (첫 번째 토큰 제외)\n",
    "        return input_seq.clone().detach(), target_seq.clone().detach()\n",
    "\n",
    "\n",
    "seq_data_length = 5  # 시퀀스 데이터 길이\n",
    "seq_length = seq_data_length -1  # 입력 시퀀스 길이\n",
    "num_sequences = 500000  # 시퀀스 데이터 개수\n",
    "\n",
    "\n",
    "# 모델 초기화\n",
    "#embed_dim = 512 # 임베딩 차원\n",
    "#num_heads = 4   # 어텐션 헤드 수\n",
    "#num_layers = 6  # 트랜스포머 블록 수\n",
    "#batch_size = 1\n",
    "#lr = 1e-6\n",
    "\n",
    "total_epoch = 100\n",
    "\n",
    "# 시퀀스 데이터 생성 및 데이터셋 객체 생성\n",
    "sequences = create_sequences(seq_length=seq_data_length, num_sequences=num_sequences)\n",
    "dataset = SequenceDataset(sequences)\n",
    "\n",
    "# 데이터셋 예시 출력\n",
    "print(\"시퀀스 데이터 예시:\", sequences[:5])\n",
    "print(\"데이터셋 크기:\", len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "variant = {}\n",
    "variant[\"state_dim\"] = 1\n",
    "variant[\"state_range\"] = np.array([-1.0, 1.0])\n",
    "variant[\"K\"] = 5\n",
    "MAX_EPISODE_LEN = 1000\n",
    "variant[\"embed_dim\"] = 512\n",
    "variant[\"n_layer\"] = 4\n",
    "variant[\"n_head\"] = 4\n",
    "variant[\"activation_function\"] = \"gelu\"\n",
    "variant[\"dropout\"] = 0.1\n",
    "variant[\"ordering\"] = 0\n",
    "variant[\"init_temperature\"] = 0.1\n",
    "variant[\"target_entropy\"] = -variant[\"state_dim\"]\n",
    "variant[\"eval_context_length\"] = 5\n",
    "variant[\"warmup_steps\"] = 1000\n",
    "variant[\"learning_rate\"] = 1e-4\n",
    "variant[\"weight_decay\"] = 5e-4\n",
    "variant[\"dataset_num_squence\"] = num_sequences\n",
    "variant[\"dataset_seq_length\"] = seq_data_length\n",
    "variant[\"train_total_epoch\"] = total_epoch\n",
    "variant[\"stocastic_policy\"] = False\n",
    "\n",
    "model = DecisionTransformer02(\n",
    "    state_dim=variant[\"state_dim\"],\n",
    "    state_range= variant[\"state_range\"],\n",
    "    max_length=variant[\"K\"],\n",
    "    eval_context_length=variant[\"eval_context_length\"],\n",
    "    max_ep_len=MAX_EPISODE_LEN,\n",
    "    hidden_size=variant[\"embed_dim\"],\n",
    "    n_layer=variant[\"n_layer\"],\n",
    "    n_head=variant[\"n_head\"],\n",
    "    n_inner=4 * variant[\"embed_dim\"],\n",
    "    activation_function=variant[\"activation_function\"],\n",
    "    n_positions=1024,\n",
    "    resid_pdrop=variant[\"dropout\"],\n",
    "    attn_pdrop=variant[\"dropout\"],\n",
    "    stochastic_policy=variant[\"stocastic_policy\"],\n",
    "    ordering=variant[\"ordering\"],\n",
    "    init_temperature=variant[\"init_temperature\"],\n",
    "    target_entropy=variant[\"target_entropy\"],\n",
    "    #state_mean=self.state_mean,\n",
    "    #state_std=self.state_std,\n",
    ").to(device=device)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_step_stochastic_01(self, loss_fn, trajs):\n",
    "    (\n",
    "        states,\n",
    "        rewards,\n",
    "        dones,\n",
    "        rtg,\n",
    "        timesteps,\n",
    "        ordering,\n",
    "        padding_mask,\n",
    "    ) = trajs\n",
    "    \n",
    "\n",
    "    states = states.to(self.device)\n",
    "    rewards = rewards.to(self.device)\n",
    "    dones = dones.to(self.device)\n",
    "    rtg = rtg.to(self.device)\n",
    "    timesteps = timesteps.to(self.device)\n",
    "    ordering = ordering.to(self.device)\n",
    "    padding_mask = padding_mask.to(self.device)\n",
    "    #print(f\"training_input {states[0]}\")\n",
    "    #print(f\"padding_mask:{padding_mask[0]}\")\n",
    "    #print(f\"timesetps:{timesteps[0]}\")\n",
    "    #print(f\"ordering:{ordering[0]}\")\n",
    "    state_target = torch.clone(states)\n",
    "\n",
    "    states_preds = self.model.forward(\n",
    "        states,\n",
    "        timesteps,\n",
    "        ordering,\n",
    "        padding_mask=padding_mask,\n",
    "    )\n",
    "\n",
    "    loss, nll, entropy = loss_fn(\n",
    "        states_preds,  # a_hat_dist\n",
    "        state_target,\n",
    "        padding_mask,\n",
    "        self.model.temperature().detach(),  # no gradient taken here\n",
    "    )\n",
    "    #print(f\"state_target : {state_target[0]}\")\n",
    "    #print(f\"state_preds : {states_preds.mean[0]}\")\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.25)\n",
    "    self.optimizer.step()\n",
    "\n",
    "    self.log_temperature_optimizer.zero_grad()\n",
    "    temperature_loss = (\n",
    "        self.model.temperature() * (entropy - self.model.target_entropy).detach()\n",
    "    )\n",
    "    temperature_loss.backward()\n",
    "    self.log_temperature_optimizer.step()\n",
    "\n",
    "    if self.scheduler is not None:\n",
    "        self.scheduler.step()\n",
    "\n",
    "    return (\n",
    "        loss.detach().cpu().item(),\n",
    "        nll.detach().cpu().item(),\n",
    "        entropy.detach().cpu().item(),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3573/2497718004.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(input_seq, dtype=torch.float), torch.tensor(target_seq, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 1.0879371, token error : 0.9026921391487122Time: 3.5508s\n",
      "Epoch [2/100], Loss: 1.0887680, token error : 0.9032276272773743Time: 3.5129s\n",
      "Epoch [3/100], Loss: 1.0859896, token error : 0.9018241167068481Time: 3.4259s\n",
      "Epoch [4/100], Loss: 1.0876862, token error : 0.9028552174568176Time: 3.4023s\n",
      "Epoch [5/100], Loss: 1.0841189, token error : 0.9013075828552246Time: 3.4664s\n",
      "Epoch [6/100], Loss: 1.0824502, token error : 0.9005835652351379Time: 3.4257s\n",
      "Epoch [7/100], Loss: 1.0806616, token error : 0.8998225331306458Time: 3.8210s\n",
      "Epoch [8/100], Loss: 1.0798710, token error : 0.8994263410568237Time: 3.5783s\n",
      "Epoch [9/100], Loss: 1.0749229, token error : 0.8974431157112122Time: 3.4474s\n",
      "Epoch [10/100], Loss: 1.0730705, token error : 0.8967191576957703Time: 3.4633s\n",
      "Epoch [11/100], Loss: 1.0686696, token error : 0.8944797515869141Time: 3.4895s\n",
      "Epoch [12/100], Loss: 1.0639864, token error : 0.8928866386413574Time: 3.4523s\n",
      "Epoch [13/100], Loss: 1.0604817, token error : 0.8914216160774231Time: 3.4490s\n",
      "Epoch [14/100], Loss: 1.0559631, token error : 0.889205813407898Time: 3.4893s\n",
      "Epoch [15/100], Loss: 1.0489517, token error : 0.8860562443733215Time: 3.5494s\n",
      "Epoch [16/100], Loss: 1.0443802, token error : 0.8842114210128784Time: 3.4918s\n",
      "Epoch [17/100], Loss: 1.0408860, token error : 0.8828901052474976Time: 3.5378s\n",
      "Epoch [18/100], Loss: 1.0310071, token error : 0.87855464220047Time: 3.4071s\n",
      "Epoch [19/100], Loss: 1.0244549, token error : 0.8757079839706421Time: 3.5800s\n",
      "Epoch [20/100], Loss: 1.0177651, token error : 0.8731791377067566Time: 3.3878s\n",
      "Epoch [21/100], Loss: 1.0111964, token error : 0.8700849413871765Time: 3.6834s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from lamb import Lamb\n",
    "import wandb\n",
    "\n",
    "# 데이터 로더 설정\n",
    "batch_size = 1024\n",
    "\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 손실 함수와 옵티마이저\n",
    "def loss_fn_stocastic(\n",
    "    s_hat_dist,\n",
    "    s,\n",
    "    attention_mask,\n",
    "    entropy_reg,\n",
    "):\n",
    "    # a_hat is a SquashedNormal Distribution\n",
    "    log_likelihood = s_hat_dist.log_likelihood(s)[attention_mask > 0].mean()\n",
    "\n",
    "    entropy = s_hat_dist.entropy().mean()\n",
    "    loss = -(log_likelihood + entropy_reg * entropy)\n",
    "\n",
    "    return (\n",
    "        loss,\n",
    "        -log_likelihood,\n",
    "        entropy,\n",
    "    )\n",
    "    \n",
    "def loss_fn_deterministic(\n",
    "    s_hat,\n",
    "    s,\n",
    "    attention_mask\n",
    "):\n",
    "    # MSE LOSS with attention_mask > 0\n",
    "    s_hat = s_hat\n",
    "    s = s\n",
    "    #print(s_hat.shape, s.shape)\n",
    "    loss = torch.mean((s_hat - s)**2)\n",
    "    return loss\n",
    "\n",
    "\n",
    "optimizer = Lamb(\n",
    "            model.parameters(),\n",
    "            lr=variant[\"learning_rate\"],\n",
    "            weight_decay=variant[\"weight_decay\"],\n",
    "            eps=1e-8,\n",
    ")\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "            optimizer, lambda steps: min((steps + 1) / variant[\"warmup_steps\"], 1)\n",
    ")\n",
    "\n",
    "\n",
    "# 학습 루프\n",
    "def train(model, train_loader, loss_fn, optimizer, scheduler, epochs=10, log_dir='./0_gpt_trained_model/'):\n",
    "    model.train()\n",
    "    timestamp = time.strftime('%Y%m%d%H%M%S', time.localtime())\n",
    "    embed_dim = variant[\"embed_dim\"]\n",
    "    num_heads = variant[\"n_head\"]\n",
    "    num_layers = variant[\"n_layer\"]\n",
    "    seq_data_length = variant[\"dataset_seq_length\"]\n",
    "    num_sequences = variant[\"dataset_num_squence\"]\n",
    "    lr = variant[\"learning_rate\"]\n",
    "    gamma = variant[\"weight_decay\"]\n",
    "    total_epoch = variant[\"train_total_epoch\"]\n",
    "    stocastic_policy = variant[\"stocastic_policy\"]\n",
    "    \n",
    "    if stocastic_policy:\n",
    "        log_temperature_optimizer = torch.optim.Adam(\n",
    "            [model.log_temperature],\n",
    "            lr=1e-4,\n",
    "            betas=[0.9, 0.999],\n",
    "        )\n",
    "    \n",
    "    model_name = f\"DT_ed{embed_dim}_nh{num_heads}_nl{num_layers}_sdl{seq_data_length}_ns{num_sequences}_lr{lr}_g{gamma}_epoch{total_epoch}_{timestamp}\"\n",
    "    log_dir = log_dir + f\"{model_name}/\"\n",
    "    try:\n",
    "        os.mkdir(log_dir)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    import os\n",
    "    try:\n",
    "        os.mkdir(log_dir)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        total_token_error = []\n",
    "        epoch_start_time = time.time()\n",
    "        total_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs.unsqueeze(-1))\n",
    "            \n",
    "            padding_mask = torch.ones((inputs.shape[0], seq_length), dtype=torch.long)\n",
    "            \n",
    "            if stocastic_policy:\n",
    "                loss, nll, entropy = loss_fn(outputs, targets.unsqueeze(-1),attention_mask=padding_mask,entropy_reg=model.temperature().detach())\n",
    "            else :\n",
    "                loss = loss_fn(outputs, targets.unsqueeze(-1),attention_mask=padding_mask)\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if stocastic_policy:\n",
    "                log_temperature_optimizer.zero_grad()\n",
    "                temperature_loss = (\n",
    "                    model.temperature() * (entropy - model.target_entropy).detach()\n",
    "                )\n",
    "                temperature_loss.backward()\n",
    "                log_temperature_optimizer.step()\n",
    "            if stocastic_policy:\n",
    "                output_sample = outputs.mean\n",
    "            else :\n",
    "                output_sample = outputs\n",
    "            token_error = torch.abs(targets.detach() - output_sample.detach().reshape(targets.shape)).mean()\n",
    "            total_token_error.append(token_error.cpu())\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            if wandb.run is not None:\n",
    "                if variant['stocastic_policy'] :\n",
    "                    wandb.log(\n",
    "                        {\n",
    "                            \"loss\": loss.item(),\n",
    "                            \"nll\": nll.item(),\n",
    "                            \"entropy\": entropy.item(),\n",
    "                            \"token_error\": token_error.item(),\n",
    "                            \"temperature\": model.temperature().item(),\n",
    "                        }\n",
    "                    )\n",
    "                else :                        \n",
    "                    wandb.log(\n",
    "                        {\n",
    "                            \"loss\": loss.item(),\n",
    "                            \"token_error\": token_error.item(),\n",
    "                        }\n",
    "                )\n",
    "        scheduler.step()\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_token_error = np.mean(total_token_error)\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        \n",
    "        if wandb.run is not None:\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"avg_loss\": avg_loss,\n",
    "                    \"epoch_token_error\": epoch_token_error,\n",
    "                }\n",
    "            )\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {avg_loss:.7f}, token error : {epoch_token_error}Time: {epoch_end_time - epoch_start_time:.4f}s\")\n",
    "        if stocastic_policy:\n",
    "            print(f\"entropy : {entropy}, temperature : {model.temperature()}\")\n",
    "        if epoch % 10 == 0:\n",
    "            model_name = f\"gpt2_ed{embed_dim}_nh{num_heads}_nl{num_layers}_sdl{seq_data_length}_ns{num_sequences}_lr{lr}_g{gamma}_epoch{total_epoch}_tte{epoch_token_error}_ep{epoch}.pt\"\n",
    "            pass\n",
    "            model.save(f\"{log_dir}{model_name}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Total Learning time : {end_time - start_time:.4f}s\")\n",
    "    return epoch_token_error\n",
    "\n",
    "from jesnk_utils.utils import get_current_time\n",
    "\n",
    "current_time = get_current_time()\n",
    "\n",
    "wandb_enable = True\n",
    "if wandb_enable :\n",
    "    wandb.init(project=\"GPT_exp\", entity=\"jesnk\", name=f\"DT02_DET_{current_time}\")\n",
    "    wandb.config.update(variant)\n",
    "    wandb.watch(model)\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = loss_fn_stocastic if variant[\"stocastic_policy\"] else loss_fn_deterministic\n",
    "last_epoch_token_error = train(model, train_loader, loss_fn, optimizer, scheduler, epochs=total_epoch)\n",
    "last_epoch_token_error = str(round(last_epoch_token_error, 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10131/931956098.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(input_seq, dtype=torch.float), torch.tensor(target_seq, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "input : tensor([0.5186, 0.5286, 0.5386, 0.5486], device='cuda:0')\n",
      "output : tensor([[0.4782],\n",
      "        [0.4845],\n",
      "        [0.4907],\n",
      "        [0.4968]], device='cuda:0')\n",
      "target : tensor([0.5286, 0.5386, 0.5486, 0.5586], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.7884, 0.7984, 0.8084, 0.8184], device='cuda:0')\n",
      "output : tensor([[0.6572],\n",
      "        [0.6618],\n",
      "        [0.6663],\n",
      "        [0.6707]], device='cuda:0')\n",
      "target : tensor([0.7984, 0.8084, 0.8184, 0.8284], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.4018, 0.4118, 0.4218, 0.4318], device='cuda:0')\n",
      "output : tensor([[0.3838],\n",
      "        [0.3907],\n",
      "        [0.3975],\n",
      "        [0.4043]], device='cuda:0')\n",
      "target : tensor([0.4118, 0.4218, 0.4318, 0.4418], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.0220, 0.0320, 0.0420, 0.0520], device='cuda:0')\n",
      "output : tensor([[0.0289],\n",
      "        [0.0368],\n",
      "        [0.0448],\n",
      "        [0.0527]], device='cuda:0')\n",
      "target : tensor([0.0320, 0.0420, 0.0520, 0.0620], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.8579, 0.8679, 0.8779, 0.8879], device='cuda:0')\n",
      "output : tensor([[0.6941],\n",
      "        [0.6982],\n",
      "        [0.7023],\n",
      "        [0.7063]], device='cuda:0')\n",
      "target : tensor([0.8679, 0.8779, 0.8879, 0.8979], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.6468, 0.6568, 0.6668, 0.6768], device='cuda:0')\n",
      "output : tensor([[0.5704],\n",
      "        [0.5759],\n",
      "        [0.5813],\n",
      "        [0.5867]], device='cuda:0')\n",
      "target : tensor([0.6568, 0.6668, 0.6768, 0.6868], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.4029, 0.4129, 0.4229, 0.4329], device='cuda:0')\n",
      "output : tensor([[0.3847],\n",
      "        [0.3916],\n",
      "        [0.3984],\n",
      "        [0.4052]], device='cuda:0')\n",
      "target : tensor([0.4129, 0.4229, 0.4329, 0.4429], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.7553, 0.7653, 0.7753, 0.7853], device='cuda:0')\n",
      "output : tensor([[0.6383],\n",
      "        [0.6431],\n",
      "        [0.6478],\n",
      "        [0.6525]], device='cuda:0')\n",
      "target : tensor([0.7653, 0.7753, 0.7853, 0.7953], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.4397, 0.4497, 0.4597, 0.4697], device='cuda:0')\n",
      "output : tensor([[0.4155],\n",
      "        [0.4222],\n",
      "        [0.4288],\n",
      "        [0.4354]], device='cuda:0')\n",
      "target : tensor([0.4497, 0.4597, 0.4697, 0.4797], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.6531, 0.6631, 0.6731, 0.6831], device='cuda:0')\n",
      "output : tensor([[0.5746],\n",
      "        [0.5801],\n",
      "        [0.5854],\n",
      "        [0.5907]], device='cuda:0')\n",
      "target : tensor([0.6631, 0.6731, 0.6831, 0.6931], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.0877, -0.0777, -0.0677, -0.0577], device='cuda:0')\n",
      "output : tensor([[-0.0791],\n",
      "        [-0.0711],\n",
      "        [-0.0632],\n",
      "        [-0.0553]], device='cuda:0')\n",
      "target : tensor([-0.0777, -0.0677, -0.0577, -0.0477], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.0545, -0.0445, -0.0345, -0.0245], device='cuda:0')\n",
      "output : tensor([[-0.0464],\n",
      "        [-0.0384],\n",
      "        [-0.0305],\n",
      "        [-0.0226]], device='cuda:0')\n",
      "target : tensor([-0.0445, -0.0345, -0.0245, -0.0145], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.0379, -0.0279, -0.0179, -0.0079], device='cuda:0')\n",
      "output : tensor([[-0.0301],\n",
      "        [-0.0221],\n",
      "        [-0.0142],\n",
      "        [-0.0063]], device='cuda:0')\n",
      "target : tensor([-0.0279, -0.0179, -0.0079,  0.0021], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.0026, 0.0126, 0.0226, 0.0326], device='cuda:0')\n",
      "output : tensor([[0.0098],\n",
      "        [0.0177],\n",
      "        [0.0257],\n",
      "        [0.0336]], device='cuda:0')\n",
      "target : tensor([0.0126, 0.0226, 0.0326, 0.0426], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.5517, 0.5617, 0.5717, 0.5817], device='cuda:0')\n",
      "output : tensor([[0.5033],\n",
      "        [0.5093],\n",
      "        [0.5153],\n",
      "        [0.5213]], device='cuda:0')\n",
      "target : tensor([0.5617, 0.5717, 0.5817, 0.5917], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.4511, -0.4411, -0.4311, -0.4211], device='cuda:0')\n",
      "output : tensor([[-0.4131],\n",
      "        [-0.4063],\n",
      "        [-0.3995],\n",
      "        [-0.3926]], device='cuda:0')\n",
      "target : tensor([-0.4411, -0.4311, -0.4211, -0.4111], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.8728, 0.8828, 0.8928, 0.9028], device='cuda:0')\n",
      "output : tensor([[0.7016],\n",
      "        [0.7056],\n",
      "        [0.7096],\n",
      "        [0.7135]], device='cuda:0')\n",
      "target : tensor([0.8828, 0.8928, 0.9028, 0.9128], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.9202, -0.9102, -0.9002, -0.8902], device='cuda:0')\n",
      "output : tensor([[-0.7172],\n",
      "        [-0.7133],\n",
      "        [-0.7093],\n",
      "        [-0.7053]], device='cuda:0')\n",
      "target : tensor([-0.9102, -0.9002, -0.8902, -0.8802], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.4686, 0.4786, 0.4886, 0.4986], device='cuda:0')\n",
      "output : tensor([[0.4389],\n",
      "        [0.4455],\n",
      "        [0.4520],\n",
      "        [0.4584]], device='cuda:0')\n",
      "target : tensor([0.4786, 0.4886, 0.4986, 0.5086], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.7887, 0.7987, 0.8087, 0.8187], device='cuda:0')\n",
      "output : tensor([[0.6574],\n",
      "        [0.6619],\n",
      "        [0.6664],\n",
      "        [0.6709]], device='cuda:0')\n",
      "target : tensor([0.7987, 0.8087, 0.8187, 0.8287], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.4564, -0.4464, -0.4364, -0.4264], device='cuda:0')\n",
      "output : tensor([[-0.4175],\n",
      "        [-0.4107],\n",
      "        [-0.4039],\n",
      "        [-0.3971]], device='cuda:0')\n",
      "target : tensor([-0.4464, -0.4364, -0.4264, -0.4164], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.8127, -0.8027, -0.7927, -0.7827], device='cuda:0')\n",
      "output : tensor([[-0.6626],\n",
      "        [-0.6580],\n",
      "        [-0.6534],\n",
      "        [-0.6487]], device='cuda:0')\n",
      "target : tensor([-0.8027, -0.7927, -0.7827, -0.7727], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.2251, 0.2351, 0.2451, 0.2551], device='cuda:0')\n",
      "output : tensor([[0.2254],\n",
      "        [0.2329],\n",
      "        [0.2405],\n",
      "        [0.2480]], device='cuda:0')\n",
      "target : tensor([0.2351, 0.2451, 0.2551, 0.2651], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.9086, -0.8986, -0.8886, -0.8786], device='cuda:0')\n",
      "output : tensor([[-0.7117],\n",
      "        [-0.7077],\n",
      "        [-0.7037],\n",
      "        [-0.6996]], device='cuda:0')\n",
      "target : tensor([-0.8986, -0.8886, -0.8786, -0.8686], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.5333, 0.5433, 0.5533, 0.5633], device='cuda:0')\n",
      "output : tensor([[0.4894],\n",
      "        [0.4956],\n",
      "        [0.5017],\n",
      "        [0.5078]], device='cuda:0')\n",
      "target : tensor([0.5433, 0.5533, 0.5633, 0.5733], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.5635, -0.5535, -0.5435, -0.5335], device='cuda:0')\n",
      "output : tensor([[-0.5014],\n",
      "        [-0.4953],\n",
      "        [-0.4891],\n",
      "        [-0.4829]], device='cuda:0')\n",
      "target : tensor([-0.5535, -0.5435, -0.5335, -0.5235], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.8780, 0.8880, 0.8980, 0.9080], device='cuda:0')\n",
      "output : tensor([[0.7041],\n",
      "        [0.7081],\n",
      "        [0.7121],\n",
      "        [0.7160]], device='cuda:0')\n",
      "target : tensor([0.8880, 0.8980, 0.9080, 0.9180], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.3623, 0.3723, 0.3823, 0.3923], device='cuda:0')\n",
      "output : tensor([[0.3498],\n",
      "        [0.3569],\n",
      "        [0.3639],\n",
      "        [0.3709]], device='cuda:0')\n",
      "target : tensor([0.3723, 0.3823, 0.3923, 0.4023], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.3431, -0.3331, -0.3231, -0.3131], device='cuda:0')\n",
      "output : tensor([[-0.3202],\n",
      "        [-0.3130],\n",
      "        [-0.3057],\n",
      "        [-0.2983]], device='cuda:0')\n",
      "target : tensor([-0.3331, -0.3231, -0.3131, -0.3031], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.3694, -0.3594, -0.3494, -0.3394], device='cuda:0')\n",
      "output : tensor([[-0.3435],\n",
      "        [-0.3363],\n",
      "        [-0.3291],\n",
      "        [-0.3219]], device='cuda:0')\n",
      "target : tensor([-0.3594, -0.3494, -0.3394, -0.3294], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.2056, -0.1956, -0.1856, -0.1756], device='cuda:0')\n",
      "output : tensor([[-0.1931],\n",
      "        [-0.1854],\n",
      "        [-0.1777],\n",
      "        [-0.1699]], device='cuda:0')\n",
      "target : tensor([-0.1956, -0.1856, -0.1756, -0.1656], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.3025, -0.2925, -0.2825, -0.2725], device='cuda:0')\n",
      "output : tensor([[-0.2836],\n",
      "        [-0.2762],\n",
      "        [-0.2687],\n",
      "        [-0.2612]], device='cuda:0')\n",
      "target : tensor([-0.2925, -0.2825, -0.2725, -0.2625], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.1201, -0.1101, -0.1001, -0.0901], device='cuda:0')\n",
      "output : tensor([[-0.1107],\n",
      "        [-0.1028],\n",
      "        [-0.0949],\n",
      "        [-0.0871]], device='cuda:0')\n",
      "target : tensor([-0.1101, -0.1001, -0.0901, -0.0801], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.2860, 0.2960, 0.3060, 0.3160], device='cuda:0')\n",
      "output : tensor([[0.2817],\n",
      "        [0.2891],\n",
      "        [0.2965],\n",
      "        [0.3038]], device='cuda:0')\n",
      "target : tensor([0.2960, 0.3060, 0.3160, 0.3260], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.8972, -0.8872, -0.8772, -0.8672], device='cuda:0')\n",
      "output : tensor([[-0.7063],\n",
      "        [-0.7022],\n",
      "        [-0.6981],\n",
      "        [-0.6940]], device='cuda:0')\n",
      "target : tensor([-0.8872, -0.8772, -0.8672, -0.8572], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.2298, 0.2398, 0.2498, 0.2598], device='cuda:0')\n",
      "output : tensor([[0.2297],\n",
      "        [0.2373],\n",
      "        [0.2448],\n",
      "        [0.2523]], device='cuda:0')\n",
      "target : tensor([0.2398, 0.2498, 0.2598, 0.2698], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.7203, -0.7103, -0.7003, -0.6903], device='cuda:0')\n",
      "output : tensor([[-0.6085],\n",
      "        [-0.6034],\n",
      "        [-0.5982],\n",
      "        [-0.5929]], device='cuda:0')\n",
      "target : tensor([-0.7103, -0.7003, -0.6903, -0.6803], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.9262, -0.9162, -0.9062, -0.8962], device='cuda:0')\n",
      "output : tensor([[-0.7200],\n",
      "        [-0.7162],\n",
      "        [-0.7122],\n",
      "        [-0.7083]], device='cuda:0')\n",
      "target : tensor([-0.9162, -0.9062, -0.8962, -0.8862], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.4356, -0.4256, -0.4156, -0.4056], device='cuda:0')\n",
      "output : tensor([[-0.4002],\n",
      "        [-0.3934],\n",
      "        [-0.3865],\n",
      "        [-0.3795]], device='cuda:0')\n",
      "target : tensor([-0.4256, -0.4156, -0.4056, -0.3956], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.8308, 0.8408, 0.8508, 0.8608], device='cuda:0')\n",
      "output : tensor([[0.6802],\n",
      "        [0.6845],\n",
      "        [0.6887],\n",
      "        [0.6929]], device='cuda:0')\n",
      "target : tensor([0.8408, 0.8508, 0.8608, 0.8708], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.2277, -0.2177, -0.2077, -0.1977], device='cuda:0')\n",
      "output : tensor([[-0.2141],\n",
      "        [-0.2064],\n",
      "        [-0.1987],\n",
      "        [-0.1910]], device='cuda:0')\n",
      "target : tensor([-0.2177, -0.2077, -0.1977, -0.1877], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.6539, 0.6639, 0.6739, 0.6839], device='cuda:0')\n",
      "output : tensor([[0.5752],\n",
      "        [0.5806],\n",
      "        [0.5860],\n",
      "        [0.5913]], device='cuda:0')\n",
      "target : tensor([0.6639, 0.6739, 0.6839, 0.6939], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.3589, 0.3689, 0.3789, 0.3889], device='cuda:0')\n",
      "output : tensor([[0.3469],\n",
      "        [0.3540],\n",
      "        [0.3610],\n",
      "        [0.3680]], device='cuda:0')\n",
      "target : tensor([0.3689, 0.3789, 0.3889, 0.3989], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.5542, 0.5642, 0.5742, 0.5842], device='cuda:0')\n",
      "output : tensor([[0.5051],\n",
      "        [0.5111],\n",
      "        [0.5171],\n",
      "        [0.5231]], device='cuda:0')\n",
      "target : tensor([0.5642, 0.5742, 0.5842, 0.5942], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.8636, -0.8536, -0.8436, -0.8336], device='cuda:0')\n",
      "output : tensor([[-0.6895],\n",
      "        [-0.6853],\n",
      "        [-0.6810],\n",
      "        [-0.6766]], device='cuda:0')\n",
      "target : tensor([-0.8536, -0.8436, -0.8336, -0.8236], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.6866, 0.6966, 0.7066, 0.7166], device='cuda:0')\n",
      "output : tensor([[0.5964],\n",
      "        [0.6017],\n",
      "        [0.6068],\n",
      "        [0.6119]], device='cuda:0')\n",
      "target : tensor([0.6966, 0.7066, 0.7166, 0.7266], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.8334, 0.8434, 0.8534, 0.8634], device='cuda:0')\n",
      "output : tensor([[0.6816],\n",
      "        [0.6858],\n",
      "        [0.6901],\n",
      "        [0.6942]], device='cuda:0')\n",
      "target : tensor([0.8434, 0.8534, 0.8634, 0.8734], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.8487, 0.8587, 0.8687, 0.8787], device='cuda:0')\n",
      "output : tensor([[0.6894],\n",
      "        [0.6936],\n",
      "        [0.6978],\n",
      "        [0.7018]], device='cuda:0')\n",
      "target : tensor([0.8587, 0.8687, 0.8787, 0.8887], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.8772, -0.8672, -0.8572, -0.8472], device='cuda:0')\n",
      "output : tensor([[-0.6964],\n",
      "        [-0.6922],\n",
      "        [-0.6880],\n",
      "        [-0.6838]], device='cuda:0')\n",
      "target : tensor([-0.8672, -0.8572, -0.8472, -0.8372], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.8892, -0.8792, -0.8692, -0.8592], device='cuda:0')\n",
      "output : tensor([[-0.7023],\n",
      "        [-0.6983],\n",
      "        [-0.6941],\n",
      "        [-0.6899]], device='cuda:0')\n",
      "target : tensor([-0.8792, -0.8692, -0.8592, -0.8492], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.3220, 0.3320, 0.3420, 0.3520], device='cuda:0')\n",
      "output : tensor([[0.3143],\n",
      "        [0.3215],\n",
      "        [0.3287],\n",
      "        [0.3359]], device='cuda:0')\n",
      "target : tensor([0.3320, 0.3420, 0.3520, 0.3620], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.5341, 0.5441, 0.5541, 0.5641], device='cuda:0')\n",
      "output : tensor([[0.4900],\n",
      "        [0.4962],\n",
      "        [0.5023],\n",
      "        [0.5084]], device='cuda:0')\n",
      "target : tensor([0.5441, 0.5541, 0.5641, 0.5741], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.4722, -0.4622, -0.4522, -0.4422], device='cuda:0')\n",
      "output : tensor([[-0.4304],\n",
      "        [-0.4237],\n",
      "        [-0.4170],\n",
      "        [-0.4102]], device='cuda:0')\n",
      "target : tensor([-0.4622, -0.4522, -0.4422, -0.4322], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.8766, -0.8666, -0.8566, -0.8466], device='cuda:0')\n",
      "output : tensor([[-0.6961],\n",
      "        [-0.6919],\n",
      "        [-0.6877],\n",
      "        [-0.6834]], device='cuda:0')\n",
      "target : tensor([-0.8666, -0.8566, -0.8466, -0.8366], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.4311, -0.4211, -0.4111, -0.4011], device='cuda:0')\n",
      "output : tensor([[-0.3964],\n",
      "        [-0.3896],\n",
      "        [-0.3827],\n",
      "        [-0.3757]], device='cuda:0')\n",
      "target : tensor([-0.4211, -0.4111, -0.4011, -0.3911], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.9470, 0.9570, 0.9670, 0.9770], device='cuda:0')\n",
      "output : tensor([[0.7362],\n",
      "        [0.7398],\n",
      "        [0.7433],\n",
      "        [0.7468]], device='cuda:0')\n",
      "target : tensor([0.9570, 0.9670, 0.9770, 0.9870], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.9423, -0.9323, -0.9223, -0.9123], device='cuda:0')\n",
      "output : tensor([[-0.7274],\n",
      "        [-0.7236],\n",
      "        [-0.7198],\n",
      "        [-0.7159]], device='cuda:0')\n",
      "target : tensor([-0.9323, -0.9223, -0.9123, -0.9023], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.1003, -0.0903, -0.0803, -0.0703], device='cuda:0')\n",
      "output : tensor([[-0.0913],\n",
      "        [-0.0834],\n",
      "        [-0.0755],\n",
      "        [-0.0676]], device='cuda:0')\n",
      "target : tensor([-0.0903, -0.0803, -0.0703, -0.0603], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.5213, 0.5313, 0.5413, 0.5513], device='cuda:0')\n",
      "output : tensor([[0.4803],\n",
      "        [0.4866],\n",
      "        [0.4928],\n",
      "        [0.4989]], device='cuda:0')\n",
      "target : tensor([0.5313, 0.5413, 0.5513, 0.5613], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.9872, -0.9772, -0.9672, -0.9572], device='cuda:0')\n",
      "output : tensor([[-0.7470],\n",
      "        [-0.7435],\n",
      "        [-0.7400],\n",
      "        [-0.7363]], device='cuda:0')\n",
      "target : tensor([-0.9772, -0.9672, -0.9572, -0.9472], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.4993, 0.5093, 0.5193, 0.5293], device='cuda:0')\n",
      "output : tensor([[0.4633],\n",
      "        [0.4696],\n",
      "        [0.4760],\n",
      "        [0.4822]], device='cuda:0')\n",
      "target : tensor([0.5093, 0.5193, 0.5293, 0.5393], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.2428, 0.2528, 0.2628, 0.2728], device='cuda:0')\n",
      "output : tensor([[0.2419],\n",
      "        [0.2494],\n",
      "        [0.2569],\n",
      "        [0.2644]], device='cuda:0')\n",
      "target : tensor([0.2528, 0.2628, 0.2728, 0.2828], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.5846, 0.5946, 0.6046, 0.6146], device='cuda:0')\n",
      "output : tensor([[0.5273],\n",
      "        [0.5331],\n",
      "        [0.5389],\n",
      "        [0.5447]], device='cuda:0')\n",
      "target : tensor([0.5946, 0.6046, 0.6146, 0.6246], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.3870, 0.3970, 0.4070, 0.4170], device='cuda:0')\n",
      "output : tensor([[0.3712],\n",
      "        [0.3781],\n",
      "        [0.3851],\n",
      "        [0.3919]], device='cuda:0')\n",
      "target : tensor([0.3970, 0.4070, 0.4170, 0.4270], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.0939, -0.0839, -0.0739, -0.0639], device='cuda:0')\n",
      "output : tensor([[-0.0851],\n",
      "        [-0.0772],\n",
      "        [-0.0693],\n",
      "        [-0.0613]], device='cuda:0')\n",
      "target : tensor([-0.0839, -0.0739, -0.0639, -0.0539], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.9135, 0.9235, 0.9335, 0.9435], device='cuda:0')\n",
      "output : tensor([[0.7210],\n",
      "        [0.7248],\n",
      "        [0.7286],\n",
      "        [0.7323]], device='cuda:0')\n",
      "target : tensor([0.9235, 0.9335, 0.9435, 0.9535], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.4224, 0.4324, 0.4424, 0.4524], device='cuda:0')\n",
      "output : tensor([[0.4011],\n",
      "        [0.4079],\n",
      "        [0.4146],\n",
      "        [0.4213]], device='cuda:0')\n",
      "target : tensor([0.4324, 0.4424, 0.4524, 0.4624], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.7086, 0.7186, 0.7286, 0.7386], device='cuda:0')\n",
      "output : tensor([[0.6103],\n",
      "        [0.6153],\n",
      "        [0.6203],\n",
      "        [0.6253]], device='cuda:0')\n",
      "target : tensor([0.7186, 0.7286, 0.7386, 0.7486], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.0920, 0.1020, 0.1120, 0.1220], device='cuda:0')\n",
      "output : tensor([[0.0976],\n",
      "        [0.1055],\n",
      "        [0.1133],\n",
      "        [0.1212]], device='cuda:0')\n",
      "target : tensor([0.1020, 0.1120, 0.1220, 0.1320], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.8029, 0.8129, 0.8229, 0.8329], device='cuda:0')\n",
      "output : tensor([[0.6652],\n",
      "        [0.6697],\n",
      "        [0.6741],\n",
      "        [0.6785]], device='cuda:0')\n",
      "target : tensor([0.8129, 0.8229, 0.8329, 0.8429], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.4639, 0.4739, 0.4839, 0.4939], device='cuda:0')\n",
      "output : tensor([[0.4352],\n",
      "        [0.4418],\n",
      "        [0.4483],\n",
      "        [0.4547]], device='cuda:0')\n",
      "target : tensor([0.4739, 0.4839, 0.4939, 0.5039], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.4033, 0.4133, 0.4233, 0.4333], device='cuda:0')\n",
      "output : tensor([[0.3851],\n",
      "        [0.3920],\n",
      "        [0.3988],\n",
      "        [0.4056]], device='cuda:0')\n",
      "target : tensor([0.4133, 0.4233, 0.4333, 0.4433], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.2182, 0.2282, 0.2382, 0.2482], device='cuda:0')\n",
      "output : tensor([[0.2188],\n",
      "        [0.2264],\n",
      "        [0.2340],\n",
      "        [0.2416]], device='cuda:0')\n",
      "target : tensor([0.2282, 0.2382, 0.2482, 0.2582], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.4263, 0.4363, 0.4463, 0.4563], device='cuda:0')\n",
      "output : tensor([[0.4044],\n",
      "        [0.4112],\n",
      "        [0.4179],\n",
      "        [0.4245]], device='cuda:0')\n",
      "target : tensor([0.4363, 0.4463, 0.4563, 0.4663], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.3202, 0.3302, 0.3402, 0.3502], device='cuda:0')\n",
      "output : tensor([[0.3127],\n",
      "        [0.3199],\n",
      "        [0.3271],\n",
      "        [0.3343]], device='cuda:0')\n",
      "target : tensor([0.3302, 0.3402, 0.3502, 0.3602], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.1046, -0.0946, -0.0846, -0.0746], device='cuda:0')\n",
      "output : tensor([[-0.0955],\n",
      "        [-0.0876],\n",
      "        [-0.0797],\n",
      "        [-0.0718]], device='cuda:0')\n",
      "target : tensor([-0.0946, -0.0846, -0.0746, -0.0646], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.9356, -0.9256, -0.9156, -0.9056], device='cuda:0')\n",
      "output : tensor([[-0.7243],\n",
      "        [-0.7205],\n",
      "        [-0.7167],\n",
      "        [-0.7127]], device='cuda:0')\n",
      "target : tensor([-0.9256, -0.9156, -0.9056, -0.8956], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.5056, 0.5156, 0.5256, 0.5356], device='cuda:0')\n",
      "output : tensor([[0.4682],\n",
      "        [0.4746],\n",
      "        [0.4808],\n",
      "        [0.4871]], device='cuda:0')\n",
      "target : tensor([0.5156, 0.5256, 0.5356, 0.5456], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.3688, 0.3788, 0.3888, 0.3988], device='cuda:0')\n",
      "output : tensor([[0.3555],\n",
      "        [0.3625],\n",
      "        [0.3695],\n",
      "        [0.3765]], device='cuda:0')\n",
      "target : tensor([0.3788, 0.3888, 0.3988, 0.4088], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.0976, -0.0876, -0.0776, -0.0676], device='cuda:0')\n",
      "output : tensor([[-0.0887],\n",
      "        [-0.0809],\n",
      "        [-0.0729],\n",
      "        [-0.0650]], device='cuda:0')\n",
      "target : tensor([-0.0876, -0.0776, -0.0676, -0.0576], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.8041, -0.7941, -0.7841, -0.7741], device='cuda:0')\n",
      "output : tensor([[-0.6578],\n",
      "        [-0.6532],\n",
      "        [-0.6485],\n",
      "        [-0.6438]], device='cuda:0')\n",
      "target : tensor([-0.7941, -0.7841, -0.7741, -0.7641], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.6316, 0.6416, 0.6516, 0.6616], device='cuda:0')\n",
      "output : tensor([[0.5602],\n",
      "        [0.5657],\n",
      "        [0.5712],\n",
      "        [0.5767]], device='cuda:0')\n",
      "target : tensor([0.6416, 0.6516, 0.6616, 0.6716], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.6799, 0.6899, 0.6999, 0.7099], device='cuda:0')\n",
      "output : tensor([[0.5921],\n",
      "        [0.5974],\n",
      "        [0.6026],\n",
      "        [0.6077]], device='cuda:0')\n",
      "target : tensor([0.6899, 0.6999, 0.7099, 0.7199], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.0485, 0.0585, 0.0685, 0.0785], device='cuda:0')\n",
      "output : tensor([[0.0550],\n",
      "        [0.0629],\n",
      "        [0.0708],\n",
      "        [0.0787]], device='cuda:0')\n",
      "target : tensor([0.0585, 0.0685, 0.0785, 0.0885], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.4284, -0.4184, -0.4084, -0.3984], device='cuda:0')\n",
      "output : tensor([[-0.3942],\n",
      "        [-0.3873],\n",
      "        [-0.3803],\n",
      "        [-0.3734]], device='cuda:0')\n",
      "target : tensor([-0.4184, -0.4084, -0.3984, -0.3884], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.9117, -0.9017, -0.8917, -0.8817], device='cuda:0')\n",
      "output : tensor([[-0.7132],\n",
      "        [-0.7093],\n",
      "        [-0.7053],\n",
      "        [-0.7012]], device='cuda:0')\n",
      "target : tensor([-0.9017, -0.8917, -0.8817, -0.8717], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.3059, -0.2959, -0.2859, -0.2759], device='cuda:0')\n",
      "output : tensor([[-0.2867],\n",
      "        [-0.2793],\n",
      "        [-0.2719],\n",
      "        [-0.2644]], device='cuda:0')\n",
      "target : tensor([-0.2959, -0.2859, -0.2759, -0.2659], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.3252, 0.3352, 0.3452, 0.3552], device='cuda:0')\n",
      "output : tensor([[0.3171],\n",
      "        [0.3243],\n",
      "        [0.3315],\n",
      "        [0.3387]], device='cuda:0')\n",
      "target : tensor([0.3352, 0.3452, 0.3552, 0.3652], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.4584, 0.4684, 0.4784, 0.4884], device='cuda:0')\n",
      "output : tensor([[0.4307],\n",
      "        [0.4373],\n",
      "        [0.4439],\n",
      "        [0.4504]], device='cuda:0')\n",
      "target : tensor([0.4684, 0.4784, 0.4884, 0.4984], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.6432, -0.6332, -0.6232, -0.6132], device='cuda:0')\n",
      "output : tensor([[-0.5583],\n",
      "        [-0.5526],\n",
      "        [-0.5469],\n",
      "        [-0.5412]], device='cuda:0')\n",
      "target : tensor([-0.6332, -0.6232, -0.6132, -0.6032], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.2477, -0.2377, -0.2277, -0.2177], device='cuda:0')\n",
      "output : tensor([[-0.2329],\n",
      "        [-0.2253],\n",
      "        [-0.2177],\n",
      "        [-0.2101]], device='cuda:0')\n",
      "target : tensor([-0.2377, -0.2277, -0.2177, -0.2077], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.3501, 0.3601, 0.3701, 0.3801], device='cuda:0')\n",
      "output : tensor([[0.3391],\n",
      "        [0.3463],\n",
      "        [0.3534],\n",
      "        [0.3604]], device='cuda:0')\n",
      "target : tensor([0.3601, 0.3701, 0.3801, 0.3901], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.3769, 0.3869, 0.3969, 0.4069], device='cuda:0')\n",
      "output : tensor([[0.3625],\n",
      "        [0.3695],\n",
      "        [0.3765],\n",
      "        [0.3834]], device='cuda:0')\n",
      "target : tensor([0.3869, 0.3969, 0.4069, 0.4169], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.7200, 0.7300, 0.7400, 0.7500], device='cuda:0')\n",
      "output : tensor([[0.6173],\n",
      "        [0.6223],\n",
      "        [0.6272],\n",
      "        [0.6321]], device='cuda:0')\n",
      "target : tensor([0.7300, 0.7400, 0.7500, 0.7600], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.4966, 0.5066, 0.5166, 0.5266], device='cuda:0')\n",
      "output : tensor([[0.4612],\n",
      "        [0.4676],\n",
      "        [0.4739],\n",
      "        [0.4802]], device='cuda:0')\n",
      "target : tensor([0.5066, 0.5166, 0.5266, 0.5366], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.0587, 0.0687, 0.0787, 0.0887], device='cuda:0')\n",
      "output : tensor([[0.0650],\n",
      "        [0.0729],\n",
      "        [0.0808],\n",
      "        [0.0886]], device='cuda:0')\n",
      "target : tensor([0.0687, 0.0787, 0.0887, 0.0987], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.5108, 0.5208, 0.5308, 0.5408], device='cuda:0')\n",
      "output : tensor([[0.4723],\n",
      "        [0.4785],\n",
      "        [0.4848],\n",
      "        [0.4910]], device='cuda:0')\n",
      "target : tensor([0.5208, 0.5308, 0.5408, 0.5508], device='cuda:0')\n",
      "====================\n",
      "input : tensor([0.4501, 0.4601, 0.4701, 0.4801], device='cuda:0')\n",
      "output : tensor([[0.4240],\n",
      "        [0.4306],\n",
      "        [0.4372],\n",
      "        [0.4438]], device='cuda:0')\n",
      "target : tensor([0.4601, 0.4701, 0.4801, 0.4901], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.8358, -0.8258, -0.8158, -0.8058], device='cuda:0')\n",
      "output : tensor([[-0.6751],\n",
      "        [-0.6706],\n",
      "        [-0.6662],\n",
      "        [-0.6616]], device='cuda:0')\n",
      "target : tensor([-0.8258, -0.8158, -0.8058, -0.7958], device='cuda:0')\n",
      "====================\n",
      "input : tensor([-0.3017, -0.2917, -0.2817, -0.2717], device='cuda:0')\n",
      "output : tensor([[-0.2829],\n",
      "        [-0.2755],\n",
      "        [-0.2680],\n",
      "        [-0.2605]], device='cuda:0')\n",
      "target : tensor([-0.2917, -0.2817, -0.2717, -0.2617], device='cuda:0')\n",
      "Test Loss: -2.6484\n",
      "Test Token Error: 0.0662\n",
      "count : 100\n"
     ]
    }
   ],
   "source": [
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_token_error = []\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for _, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            #print(inputs.shape, targets.shape)\n",
    "            outputs = model(inputs.unsqueeze(-1))\n",
    "            padding_mask = torch.ones((inputs.shape[0], seq_length), dtype=torch.long)\n",
    "\n",
    "            loss, nll, entropy = loss_fn(outputs, targets.unsqueeze(-1),attention_mask=padding_mask,entropy_reg=model.temperature().detach())\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            output_sample = outputs.mean\n",
    "            token_error = torch.abs(targets.detach() - output_sample.detach().reshape(targets.shape)).mean()\n",
    "            print(\"=\"*20)\n",
    "            print(f\"input : {inputs[0]}\")\n",
    "            print(f\"output : {outputs.mean[0]}\")\n",
    "            print(f\"target : {targets[0]}\")\n",
    "            total_token_error.append(token_error.cpu())\n",
    "            count += 1\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    avg_token_error = np.mean(total_token_error)\n",
    "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Test Token Error: {avg_token_error:.4f}\")\n",
    "    print(f\"count : {count}\")\n",
    "\n",
    "# 테스트 데이터 생성 및 데이터셋 객체 생성\n",
    "test_sequences = create_sequences(seq_length=seq_data_length, num_sequences=100000)  # 예: 200개의 테스트 시퀀스 생성\n",
    "test_dataset = SequenceDataset(test_sequences)\n",
    "\n",
    "# 데이터 로더 설정\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "# 손실 함수와 옵티마이저\n",
    "\n",
    "# 테스트 실행\n",
    "test(model, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
