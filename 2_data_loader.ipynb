{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def _load_dataset_minari(env_name): # jensk\n",
    "    import minari\n",
    "    if env_name == 'PointMaze_UMaze-v3' :\n",
    "        env_name = 'pointmaze-umaze-v1'\n",
    "    minari.download_dataset(env_name)\n",
    "    dataset = minari.load_dataset(env_name, download=True)\n",
    "    trajectories = dataset._data.get_episodes(dataset.episode_indices)\n",
    "    states, traj_lens, returns = [], [], []\n",
    "    if 'pointmaze' in env_name :\n",
    "        # re-label observation. (achieved_goal, desired_goal) -> observation\n",
    "        print(\"re-label observation. (achieved_goal, desired_goal) -> observation\")\n",
    "        for path in trajectories :\n",
    "            achieved_goal = path['observations']['achieved_goal'][1:]\n",
    "            desired_goal = path['observations']['desired_goal'][1:]\n",
    "            observation = np.concatenate([achieved_goal, desired_goal], axis=1)\n",
    "            path['observations'] = observation\n",
    "\n",
    "    for path in trajectories:\n",
    "        states.append(path[\"observations\"])\n",
    "        traj_lens.append(len(path[\"observations\"]))\n",
    "        returns.append(path[\"rewards\"].sum())\n",
    "        # for pointmaze\n",
    "    traj_lens, returns = np.array(traj_lens), np.array(returns)\n",
    "    states = np.concatenate(states, axis=0)\n",
    "    state_mean, state_std = np.mean(states, axis=0), np.std(states, axis=0) + 1e-6\n",
    "    num_timesteps = sum(traj_lens)\n",
    "\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Starting new experiment: {env_name}\")\n",
    "    print(f\"{len(traj_lens)} trajectories, {num_timesteps} timesteps found\")\n",
    "    print(f\"Average return: {np.mean(returns):.2f}, std: {np.std(returns):.2f}\")\n",
    "    print(f\"Max return: {np.max(returns):.2f}, min: {np.min(returns):.2f}\")\n",
    "    print(f\"Average length: {np.mean(traj_lens):.2f}, std: {np.std(traj_lens):.2f}\")\n",
    "    print(f\"Max length: {np.max(traj_lens):.2f}, min: {np.min(traj_lens):.2f}\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    sorted_inds = np.argsort(returns)  # lowest to highest\n",
    "    num_trajectories = 1\n",
    "    timesteps = traj_lens[sorted_inds[-1]]\n",
    "    ind = len(trajectories) - 2\n",
    "    while ind >= 0 and timesteps + traj_lens[sorted_inds[ind]] < num_timesteps:\n",
    "        timesteps += traj_lens[sorted_inds[ind]]\n",
    "        num_trajectories += 1\n",
    "        ind -= 1\n",
    "    sorted_inds = sorted_inds[-num_trajectories:]\n",
    "\n",
    "    trajectories = [trajectories[ii] for ii in sorted_inds]\n",
    "    return trajectories, state_mean, state_std\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/env/lib/python3.8/site-packages/minari/storage/hosting.py:167: UserWarning: \u001b[33mWARN: Skipping Download. Dataset pointmaze-umaze-v1 found locally at /root/.minari/datasets/pointmaze-umaze-v1, Use force_download=True to download the dataset again.\n",
      "\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re-label observation. (achieved_goal, desired_goal) -> observation\n",
      "==================================================\n",
      "Starting new experiment: pointmaze-umaze-v1\n",
      "13289 trajectories, 999996 timesteps found\n",
      "Average return: 1.00, std: 0.02\n",
      "Max return: 1.00, min: 0.00\n",
      "Average length: 75.25, std: 45.52\n",
      "Max length: 193.00, min: 0.00\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "offline_trajs, state_mean, state_std = _load_dataset_minari('PointMaze_UMaze-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SubTrajectory(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        trajectories,\n",
    "        sampling_ind,\n",
    "        transform=None,\n",
    "    ):\n",
    "\n",
    "        super(SubTrajectory, self).__init__()\n",
    "        self.sampling_ind = sampling_ind\n",
    "        self.trajs = trajectories\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        \n",
    "        traj = self.trajs[self.sampling_ind[index]]\n",
    "        if self.transform:\n",
    "            return self.transform(traj)\n",
    "        else:\n",
    "            return traj\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sampling_ind)\n",
    "\n",
    "import random\n",
    "MAX_EPISODE_LEN = 1000\n",
    "\n",
    "\n",
    "\n",
    "def discount_cumsum(x, gamma):\n",
    "    ret = np.zeros_like(x)\n",
    "    ret[-1] = x[-1]\n",
    "    for t in reversed(range(x.shape[0] - 1)):\n",
    "        ret[t] = x[t] + gamma * ret[t + 1]\n",
    "    return ret\n",
    "\n",
    "\n",
    "class TransformSamplingSubTraj01:\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_len,\n",
    "        state_dim,\n",
    "        state_mean,\n",
    "        state_std,\n",
    "        reward_scale,\n",
    "        state_range,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.max_len = max_len\n",
    "        self.state_dim = state_dim\n",
    "        self.state_mean = state_mean\n",
    "        self.state_std = state_std\n",
    "        self.reward_scale = reward_scale\n",
    "\n",
    "        # For some datasets there are actions with values 1.0/-1.0 which is problematic\n",
    "        # for the SquahsedNormal distribution. The inversed tanh transformation will\n",
    "        # produce NAN when computing the log-likelihood. We clamp them to be within\n",
    "        # the user defined action range.\n",
    "        self.state_range = state_range\n",
    "\n",
    "    def __call__(self, traj):\n",
    "        \n",
    "        si = random.randint(0, traj[\"rewards\"].shape[0] - 1)\n",
    "        \n",
    "        \n",
    "        # get sequences from dataset\n",
    "        ss = traj[\"observations\"][si : si + self.max_len].reshape(-1, self.state_dim)\n",
    "        rr = traj[\"rewards\"][si : si + self.max_len].reshape(-1, 1)\n",
    "\n",
    "        if \"terminals\" in traj:\n",
    "            dd = traj[\"terminals\"][si : si + self.max_len]  # .reshape(-1)\n",
    "        elif \"terminations\" in traj:\n",
    "            dd = traj[\"terminations\"][si : si + self.max_len]  # .reshape(-1)\n",
    "        else :\n",
    "            dd = traj[\"dones\"][si : si + self.max_len]  # .reshape(-1)\n",
    "\n",
    "        # get the total length of a trajectory\n",
    "        tlen = ss.shape[0]\n",
    "\n",
    "\n",
    "        timesteps = np.arange(si, si + tlen)  # .reshape(-1)\n",
    "        ordering = np.arange(tlen)\n",
    "        ordering[timesteps >= MAX_EPISODE_LEN] = -1\n",
    "        ordering[ordering == -1] = ordering.max()\n",
    "        timesteps[timesteps >= MAX_EPISODE_LEN] = MAX_EPISODE_LEN - 1  # padding cutoff\n",
    "\n",
    "        rtg = discount_cumsum(traj[\"rewards\"][si:], gamma=1.0)[: tlen + 1].reshape(\n",
    "            -1, 1\n",
    "        )\n",
    "        if rtg.shape[0] <= tlen:\n",
    "            rtg = np.concatenate([rtg, np.zeros((1, 1))])\n",
    "\n",
    "        # padding and state + reward normalization\n",
    "        state_len = ss.shape[0]\n",
    "        if tlen != state_len:\n",
    "            raise ValueError\n",
    "\n",
    "        ss = np.concatenate([np.zeros((self.max_len - tlen, self.state_dim)), ss])\n",
    "        #ss = (ss - self.state_mean) / self.state_std\n",
    "        # manaul normalization\n",
    "        manual_normalization = True\n",
    "        if manual_normalization :\n",
    "            manual_std = 2\n",
    "            ep = 1e-7\n",
    "            ss = ss + ep / manual_std\n",
    "        # jesnk: do not normalize state?\n",
    "\n",
    "        rr = np.concatenate([np.zeros((self.max_len - tlen, 1)), rr])\n",
    "        dd = np.concatenate([np.ones((self.max_len - tlen)) * 2, dd])\n",
    "        rtg = (\n",
    "            np.concatenate([np.zeros((self.max_len - tlen, 1)), rtg])\n",
    "            * self.reward_scale\n",
    "        )\n",
    "        \n",
    "        \n",
    "        #print(f'{(rtg.shape[0] + self.max_len - tlen)} == {self.max_len})')\n",
    "        \n",
    "        timesteps = np.concatenate([np.zeros((self.max_len - tlen)), timesteps])\n",
    "        ordering = np.concatenate([np.zeros((self.max_len - tlen)), ordering])\n",
    "        padding_mask = np.concatenate([np.zeros(self.max_len - tlen), np.ones(tlen)])\n",
    "\n",
    "        ss = torch.from_numpy(ss).to(dtype=torch.float32).clamp(*self.state_range)\n",
    "        rr = torch.from_numpy(rr).to(dtype=torch.float32)\n",
    "        dd = torch.from_numpy(dd).to(dtype=torch.long)\n",
    "        rtg = torch.from_numpy(rtg).to(dtype=torch.float32)\n",
    "        timesteps = torch.from_numpy(timesteps).to(dtype=torch.long)\n",
    "        ordering = torch.from_numpy(ordering).to(dtype=torch.long)\n",
    "        padding_mask = torch.from_numpy(padding_mask)\n",
    "\n",
    "        return ss, rr, dd, rtg, timesteps, ordering, padding_mask\n",
    "\n",
    "\n",
    "def sample_trajs(trajectories, sample_size):\n",
    "\n",
    "    traj_lens = np.array([len(traj[\"observations\"]) for traj in trajectories])\n",
    "    p_sample = traj_lens / np.sum(traj_lens)\n",
    "\n",
    "    for idx, traj in enumerate(trajectories) :\n",
    "        if len(traj['observations']) < 5 :\n",
    "            # set p to 0\n",
    "            p_sample[idx] = 0\n",
    "    # set sum(p_sample) to 1\n",
    "    p_sample = p_sample / np.sum(p_sample)\n",
    "    \n",
    "    \n",
    "    inds = np.random.choice(\n",
    "        np.arange(len(trajectories)),\n",
    "        size=sample_size,\n",
    "        replace=True,\n",
    "        p=p_sample,\n",
    "    )\n",
    "    return inds\n",
    "\n",
    "def create_dataloader_01(\n",
    "    trajectories,\n",
    "    num_iters,\n",
    "    batch_size,\n",
    "    max_len,\n",
    "    state_dim,\n",
    "    state_mean,\n",
    "    state_std,\n",
    "    reward_scale,\n",
    "    state_range,\n",
    "    num_workers=24,\n",
    "):\n",
    "    # total number of subt-rajectories you need to sample\n",
    "    sample_size = batch_size * num_iters\n",
    "    sampling_ind = sample_trajs(trajectories, sample_size)\n",
    "    transform = TransformSamplingSubTraj01(\n",
    "        max_len=max_len,\n",
    "        state_dim=state_dim,\n",
    "        state_mean=state_mean,\n",
    "        state_std=state_std,\n",
    "        reward_scale=reward_scale,\n",
    "        state_range=state_range,\n",
    "    )\n",
    "\n",
    "    subset = SubTrajectory(trajectories, sampling_ind=sampling_ind, transform=transform)\n",
    "\n",
    "    return torch.utils.data.DataLoader(\n",
    "        subset, batch_size=batch_size, num_workers=num_workers, shuffle=False\n",
    "    )\n",
    "\n",
    "dataloader = create_dataloader_01(\n",
    "    trajectories=offline_trajs,\n",
    "    num_iters=1,\n",
    "    batch_size=256,\n",
    "    max_len=5,\n",
    "    state_dim=4,\n",
    "    state_mean=[state_mean],\n",
    "    state_std=state_std,\n",
    "    reward_scale=0.001,\n",
    "    state_range= [-2,2]\n",
    ")\n",
    "\n",
    "\n",
    "transform = TransformSamplingSubTraj01(\n",
    "    max_len=5,\n",
    "    state_dim=4,\n",
    "    state_mean=state_mean,\n",
    "    state_std=state_std,\n",
    "    reward_scale=0.01,\n",
    "    state_range=[-2,2],\n",
    ")\n",
    "#transform_result = transform(offline_trajs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 5, 4])\n",
      "torch.Size([256, 5, 1])\n",
      "torch.Size([256, 5])\n",
      "torch.Size([256, 6, 1])\n",
      "torch.Size([256, 5])\n",
      "torch.Size([256, 5])\n",
      "torch.Size([256, 5])\n"
     ]
    }
   ],
   "source": [
    "# iterate over data\n",
    "for _, (ss, rr, dd, rtg, timesteps, ordering, padding_mask) in enumerate(dataloader):\n",
    "    print(ss.shape)\n",
    "    print(rr.shape)\n",
    "    print(dd.shape)\n",
    "    print(rtg.shape)\n",
    "    print(timesteps.shape)\n",
    "    print(ordering.shape)\n",
    "    print(padding_mask.shape)\n",
    "    if _ > 10 :\n",
    "        break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fd9ff5f6d90>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get sequences from dataset\n",
    "dataloader = create_dataloader_01(\n",
    "    trajectories=offline_trajs,\n",
    "    num_iters=1,\n",
    "    batch_size=256,\n",
    "    max_len=5,\n",
    "    state_dim=4,\n",
    "    state_mean=[state_mean],\n",
    "    state_std=state_std,\n",
    "    reward_scale=0.001,\n",
    "    state_range= [-2,2]\n",
    ")\n",
    "for "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
