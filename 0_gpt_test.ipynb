{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시퀀스 데이터 예시: [[ 0.67484727  0.68484727  0.69484727  0.70484727  0.71484727]\n",
      " [ 0.361735    0.371735    0.381735    0.391735    0.401735  ]\n",
      " [ 0.5510735   0.5610735   0.5710735   0.5810735   0.5910735 ]\n",
      " [-0.15488457 -0.14488457 -0.13488457 -0.12488457 -0.11488457]\n",
      " [-0.80074972 -0.79074972 -0.78074972 -0.77074972 -0.76074972]]\n",
      "데이터셋 크기: 100000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# 시퀀스 데이터 생성 함수\n",
    "def create_sequences(seq_length=5, num_sequences=1000, start=-1.0, end=1.0, step=0.01):\n",
    "    sequences = []\n",
    "    while len(sequences) < num_sequences:\n",
    "        start_val = random.uniform(start, end - seq_length * step)\n",
    "        sequence = [start_val + i * step for i in range(seq_length)]\n",
    "        if all(-1 <= x <= 1 for x in sequence):  # 시퀀스 내 모든 값이 -1과 1 사이인지 확인\n",
    "            sequences.append(sequence)\n",
    "    return np.array(sequences)\n",
    "\n",
    "# 데이터셋 클래스\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences):\n",
    "        self.sequences = sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sequence = self.sequences[idx]\n",
    "        input_seq = sequence[:-1]  # 입력 시퀀스 (마지막 토큰 제외)\n",
    "        target_seq = sequence[1:]  # 타겟 시퀀스 (첫 번째 토큰 제외)\n",
    "        return torch.tensor(input_seq, dtype=torch.float), torch.tensor(target_seq, dtype=torch.float)\n",
    "\n",
    "\n",
    "seq_data_length = 5  # 시퀀스 데이터 길이\n",
    "seq_length = seq_data_length -1  # 입력 시퀀스 길이\n",
    "num_sequences = 100000  # 시퀀스 데이터 개수\n",
    "\n",
    "\n",
    "# 모델 초기화\n",
    "embed_dim = 512 # 임베딩 차원\n",
    "num_heads = 4   # 어텐션 헤드 수\n",
    "num_layers = 6  # 트랜스포머 블록 수\n",
    "batch_size = 1\n",
    "lr = 1e-6\n",
    "\n",
    "total_epoch = 100\n",
    "gamma = 0.95\n",
    "\n",
    "# 시퀀스 데이터 생성 및 데이터셋 객체 생성\n",
    "sequences = create_sequences(seq_length=seq_data_length, num_sequences=num_sequences)\n",
    "\n",
    "\n",
    "dataset = SequenceDataset(sequences)\n",
    "\n",
    "# 데이터셋 예시 출력\n",
    "print(\"시퀀스 데이터 예시:\", sequences[:5])\n",
    "print(\"데이터셋 크기:\", len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.0001'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2(\n",
       "  (blocks): ModuleList(\n",
       "    (0): GPT2Block(\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): GPT2Block(\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (2): GPT2Block(\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): GPT2Block(\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (4): GPT2Block(\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): GPT2Block(\n",
       "      (attention): MultiheadAttention(\n",
       "        (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feed_forward): Sequential(\n",
       "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "      )\n",
       "      (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (final_layer): Linear(in_features=512, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GPT2Block(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(GPT2Block, self).__init__()\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 4 * embed_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * embed_dim, embed_dim)\n",
    "        )\n",
    "        self.layer_norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.layer_norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 멀티-헤드 어텐션\n",
    "        attn_output, _ = self.attention(x, x, x)\n",
    "        x = x + attn_output\n",
    "        x = self.layer_norm1(x)\n",
    "\n",
    "        # 피드-포워드 네트워크\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = x + ff_output\n",
    "        x = self.layer_norm2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "class GPT2(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, num_layers, seq_length):\n",
    "        super(GPT2, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.positional_embeddings = nn.Parameter(torch.randn(seq_length, embed_dim))\n",
    "        self.blocks = nn.ModuleList([GPT2Block(embed_dim, num_heads) for _ in range(num_layers)])\n",
    "        self.final_layer = nn.Linear(embed_dim, 1)  # 시퀀스의 각 위치에 대한 값을 예측\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.positional_embeddings[:x.size(1), :]\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.final_layer(x)\n",
    "        return x.squeeze(-1)\n",
    "    def save(self, save_path):\n",
    "        try:\n",
    "            torch.save({\n",
    "                'model_state_dict': self.state_dict(),\n",
    "                'model_class': self.__class__,\n",
    "                'model_args': {'embed_dim': self.embed_dim, 'num_heads': num_heads, 'num_layers': num_layers, 'seq_length': seq_length}\n",
    "            }, save_path)\n",
    "        except Exception as e:\n",
    "            print(f\"모델 저장 중 오류 발생: {e}\")\n",
    "\n",
    "    def load(self, load_path, device):\n",
    "        try:\n",
    "            checkpoint = torch.load(load_path, map_location=device)\n",
    "            model_class = checkpoint['model_class']\n",
    "            model_args = checkpoint['model_args']\n",
    "            model = model_class(**model_args).to(device)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\"모델 로드 중 오류 발생: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = GPT2(embed_dim, num_heads, num_layers, seq_length)\n",
    "model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 63\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m epoch_token_error\n\u001b[1;32m     61\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 63\u001b[0m last_epoch_token_error \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_epoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m last_epoch_token_error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mround\u001b[39m(last_epoch_token_error, \u001b[38;5;241m9\u001b[39m))\n\u001b[1;32m     65\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2_ed\u001b[39m\u001b[38;5;132;01m{\u001b[39;00membed_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_nh\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_heads\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_nl\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_sdl\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseq_data_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_ns\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_sequences\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_lr\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_g\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgamma\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_epoch\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_tte\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlast_epoch_token_error\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimestamp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[4], line 41\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer, scheduler, epochs, log_dir)\u001b[0m\n\u001b[1;32m     38\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m     40\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 41\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m token_error \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(targets\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m-\u001b[39m outputs\u001b[38;5;241m.\u001b[39mdetach())\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     43\u001b[0m total_token_error\u001b[38;5;241m.\u001b[39mappend(token_error\u001b[38;5;241m.\u001b[39mcpu())\n",
      "File \u001b[0;32m/env/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     64\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/env/lib/python3.8/site-packages/torch/optim/optimizer.py:89\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/env/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/env/lib/python3.8/site-packages/torch/optim/adam.py:108\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    107\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 108\u001b[0m     \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m           \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/env/lib/python3.8/site-packages/torch/optim/_functional.py:92\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     90\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(bias_correction2))\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbias_correction2\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m     94\u001b[0m step_size \u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m/\u001b[39m bias_correction1\n\u001b[1;32m     96\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "# 데이터 로더 설정\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 손실 함수와 옵티마이저\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=gamma)\n",
    "\n",
    "# 학습 루프\n",
    "def train(model, train_loader, criterion, optimizer, scheduler, epochs=10, log_dir='./0_gpt_trained_model/'):\n",
    "    model.train()\n",
    "    timestamp = time.strftime('%Y%m%d%H%M%S', time.localtime())\n",
    "    model_name = f\"gpt2_ed{embed_dim}_nh{num_heads}_nl{num_layers}_sdl{seq_data_length}_ns{num_sequences}_lr{lr}_g{gamma}_epoch{total_epoch}_{timestamp}\"\n",
    "    log_dir = log_dir + f\"{model_name}/\"\n",
    "    # make directory log_dir/model_name\n",
    "    import os\n",
    "    try:\n",
    "        os.mkdir(log_dir)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_token_error = []\n",
    "        epoch_start_time = time.time()\n",
    "        total_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs.unsqueeze(-1))\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            token_error = torch.abs(targets.detach() - outputs.detach()).mean()\n",
    "            total_token_error.append(token_error.cpu())\n",
    "\n",
    "\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        scheduler.step()\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_token_error = np.mean(total_token_error)\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {avg_loss:.7f}, token error : {epoch_token_error}Time: {epoch_end_time - epoch_start_time:.4f}s\")\n",
    "        if epoch % 10 == 0:\n",
    "            model_name = f\"gpt2_ed{embed_dim}_nh{num_heads}_nl{num_layers}_sdl{seq_data_length}_ns{num_sequences}_lr{lr}_g{gamma}_epoch{total_epoch}_tte{epoch_token_error}_ep{epoch}.pt\"\n",
    "            model.save(f\"{log_dir}{model_name}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Total Learning time : {end_time - start_time:.4f}s\")\n",
    "    return epoch_token_error\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "last_epoch_token_error = train(model, train_loader, criterion, optimizer, scheduler, epochs=total_epoch)\n",
    "last_epoch_token_error = str(round(last_epoch_token_error, 9))\n",
    "model_name = f\"gpt2_ed{embed_dim}_nh{num_heads}_nl{num_layers}_sdl{seq_data_length}_ns{num_sequences}_lr{lr}_g{gamma}_epoch{total_epoch}_tte{last_epoch_token_error}_{timestamp}.pt\"\n",
    "model.save(f\"./0_gpt_trained_model/{model_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = False \n",
    "\n",
    "if load :\n",
    "    embed_dim = 2048 # 임베딩 차원\n",
    "    num_heads = 4   # 어텐션 헤드 수\n",
    "    num_layers = 6  # 트랜스포머 블록 수\n",
    "    seq_data_length = 5  # 시퀀스 데이터 길이\n",
    "    seq_length = seq_data_length - 1  # 입력 시퀀스 길이\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    load_model_name = f\"./0_gpt_trained_model/gpt2_{embed_dim}_{num_heads}_{num_layers}_{seq_data_length}.pt\"\n",
    "    model = GPT2(embed_dim, num_heads, num_layers, seq_length).to(device)\n",
    "    model.load(load_model_name, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0019\n",
      "Test Token Error: 0.0365\n",
      "count : 100\n"
     ]
    }
   ],
   "source": [
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_token_error = []\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for _, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            #print(inputs.shape, targets.shape)\n",
    "            outputs = model(inputs.unsqueeze(-1))\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            #print(f\"Input: {inputs[0].tolist()}, \\nTarget: {targets[0].tolist()}, \\nPrediction: {outputs[0].tolist()}\")\n",
    "            # each token error mean\n",
    "            #print(f\"Error: {torch.abs(targets[0] - outputs[0]).mean():.4f}\\n\")\n",
    "            # token error mean\n",
    "            token_error = torch.abs(targets - outputs).mean()\n",
    "\n",
    "            total_token_error.append(token_error.cpu())\n",
    "            count += 1\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    avg_token_error = np.mean(total_token_error)\n",
    "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Test Token Error: {avg_token_error:.4f}\")\n",
    "    print(f\"count : {count}\")\n",
    "\n",
    "# 테스트 데이터 생성 및 데이터셋 객체 생성\n",
    "test_sequences = create_sequences(seq_length=seq_data_length, num_sequences=100000)  # 예: 200개의 테스트 시퀀스 생성\n",
    "test_dataset = SequenceDataset(test_sequences)\n",
    "\n",
    "# 데이터 로더 설정\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "# 손실 함수와 옵티마이저\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "\n",
    "# 테스트 실행\n",
    "test(model, test_loader, criterion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Token Error: \n",
    "## embed_size : 512\n",
    "\n",
    "* seq 41 : 0.0109\n",
    "* seq 21 : 0.0176\n",
    "* seq 5 : 0.0191\n",
    "* seq 3 : 0.0192\n",
    "\n",
    "## embed_size : 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# 각 토큰에 대해 다음 토큰 예측\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m test_tokens:\n\u001b[0;32m---> 15\u001b[0m     predicted_next_token \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_next_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m입력 토큰: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoken\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, 예측된 다음 토큰: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_next_token\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m, in \u001b[0;36mpredict_next_token\u001b[0;34m(model, token)\u001b[0m\n\u001b[1;32m      5\u001b[0m token_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[token]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 모델의 예측\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m predicted_token \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m predicted_token\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/env/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slow_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 889\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mchain(\n\u001b[1;32m    891\u001b[0m         _global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m    892\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[1;32m    893\u001b[0m     hook_result \u001b[38;5;241m=\u001b[39m hook(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, result)\n",
      "Cell \u001b[0;32mIn[3], line 35\u001b[0m, in \u001b[0;36mGPT2.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 35\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositional_embeddings\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m     37\u001b[0m         x \u001b[38;5;241m=\u001b[39m block(x)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "def predict_next_token(model, token):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # 입력 차원을 (1, 1, 1)로 변경\n",
    "        token_tensor = torch.tensor([[token]], dtype=torch.float).unsqueeze(-1)\n",
    "        # 모델의 예측\n",
    "        predicted_token = model(token_tensor)\n",
    "        return predicted_token.item()\n",
    "\n",
    "# 테스트용 단일 토큰 예시\n",
    "test_tokens = [0.005, -0.010, 0.500, -0.700, 0.999]\n",
    "\n",
    "# 각 토큰에 대해 다음 토큰 예측\n",
    "for token in test_tokens:\n",
    "    predicted_next_token = predict_next_token(model, token)\n",
    "    print(f\"입력 토큰: {token}, 예측된 다음 토큰: {predicted_next_token}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
